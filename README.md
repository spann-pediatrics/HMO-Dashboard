# Overview
This project provides a clean and reproducible workflow for processing Human Milk Oligosaccharide (HMO) data from multiple studies.
It takes raw Excel reports (which vary in structure and naming), detects the correct HMO sheet, cleans datasheets, and produces consistent, analysis-ready files. Metadata files are also identified, screened, and cleaned for "core" analysis.

The goal is to make HMO data comparable across studies so it can be analyzed in tools like Streamlit, Tableau, Power BI, or Python.

## Project Workflow 
1. Raw HMO and metadata Excel files are placed in the /raw/ folder, grouped by study
(e.g., Oxford, Brooklyn, NeoBANK).
2. New studies are added by creating a new subfolder inside /raw/ and uploading:
 - One Excel/CSV file containing HMO area counts
 - One Excel/CSV file containing associated metadata
3. The HMO processing pipeline (data_processing.ipynb) scans the raw files, identifies valid HMO sheets, standardizes column names, and saves cleaned outputs to the staging/ folder under the matching study name.
4. All cleaned HMO files are merged into a master dataset: staging/_merged/hmo_merged.csv
5. Metadata files are identified and standardized. Core identifiers and priority fields (e.g., study week, maternal age, gestational age, lactation week postpartum) are saved as: metadata__core_cleaned.csv within each study’s folder in staging/


## Folder Descriptions

#### 1. raw/ — Raw Input Files

- Contains original Excel or CSV files exactly as provided by each study. ** These files are never edited **
- Each study should have its own subfolder (e.g., Oxford, Brooklyn) containing:
  - HMO area count files
  - Metadata files (kept separate from HMO files)

#### 2. study extras/ — Manual Reference Files
These files support dashboard visualization and must be updated manually.
- study_locations: Update with the same StudyID used in HMO data when adding new studies (used for map visualizations)
- study_descriptions: Short textual descriptions of each study’s focus

#### 3. staging/ — Cleaned Outputs
Contains processed data generated by the notebooks.
- One folder per study with cleaned CSV files
 - _merged/ contains:
   - hmo_merged.csv — standardized HMO data across all studies

#### 4. derived/ - Merged HMO + Metadata 
This folder is used directly for analysis and visualization.
- The merged HMO + metadata dataset & The master metadata CSV

#### 5. catalog/ — Automated Logs
This folder is automatically updated by the pipeline. Check this as a sanity measure for what files are being read.

#### 5. dashboard/ - Steamlit
- app.py — controls all dashboard logic and visualizations


## File Descriptions

#### 1. data_processing.ipynb — Main HMO Pipeline Notebook
- Detecting HMO area count sheets within each /raw study subfolder
- Standardizing column names and renaming HMO columns to a consistent format
- Saving cleaned HMO tables into the staging/ directory
- Logging detection results and processing status into the catalog/ folder
- Merging all cleaned HMO data into a single datasetDetecting HMO area count sheet called hmo_merged.csv 

#### 2.metadataprocessing.ipynb - Metadata Pipeline Notebook
- Scanning each study’s /raw metadata files (Excel or CSV) and identifying metadata sheets
- Loading metadata sheets and normalizing column names (lowercase, stripped, standardized)
- Identifying candidate metadata fields even when naming conventions differ across studies (e.g., mat_age, maternal age, mother_age)
- Mapping study-specific column names to a shared canonical metadata schema
- Prioritizing high-confidence matches and flagging ambiguous or missing fields
- Preserving extra study-specific metadata without discarding information
- Logging all column matches, renames, and unresolved fields for transparency
- Producing: Cleaned, per-study metadata tables in staging/, Resolution logs describing how each metadata field was interpreted, A harmonized metadata dataset ready for merging with HMO data

## How to Use the Project

### 1. Add new raw files
- Place new HMO and Metadata (2 seperate) Excel or CSV files into the appropriate study folder inside /raw/
- Each study should have its own subfolder (e.g., raw/Oxford/, raw/Brooklyn/)
- Do not rename columns to match other studies — column harmonization is handled automatically

### 2. Run the data processing notebook
- Open data_processing.ipynb and run all cells. Mid-point code check should print: 'Merged X file(s) → staging/_merged/hmo_merged.csv (XXXX rows, XX columns)'
- Open metadata_processing.ipynb and run all cells.

### 3. Review Processed Outputs 
- Cleaned per-study data appear in: staging/<study_name>/
- Merged datasets appear in: staging/_merged/ (hmo), derived/ (metadata + hmo, metadata)
- Processing and resolution logs appear in: catalog/

### 4. Update Dashboard Reference Metadata (Manual Step)
- Some dashboard reference files are maintained manually and must be updated when new studies are added: Study descriptions, Geographic location metadata, Display names or study-level annotations
- These files live in the dashboard’s study extras Excel folders and are not auto-generated.

### 5. Launch the Streamlit Dashboard
- From the VS Code terminal:
"""
 cd dashboard
streamlit run app.py
"""
- The dashboard should automatically open in a browser

### 6. Best Practices
- Always rerun both notebooks when adding new studies
- Never manually edit merged CSV files
- If something looks missing in the dashboard: Check /raw/ placement, Rerun the relevant notebook, Review logs in catalog/



