{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e9c0a0",
   "metadata": {},
   "source": [
    "# Metadata Identification and Cleanings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa6d41",
   "metadata": {},
   "source": [
    "#### Step 1: Build a metadata file catalog\n",
    "1. What files exist per study in the /raw folder? Which are metadata files?\n",
    "2. For the likely metadata files, what's inside? Sheet names, column headers, row/col counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5926544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951908d",
   "metadata": {},
   "source": [
    "### Step 1. Identify Files and Which are Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a88d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the raw data directory to the \"raw\" folder\n",
    "\n",
    "RAW_DIR = Path(\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513526c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate all Excel files under /raw (recursive)\n",
    "\n",
    "excel_files = sorted(RAW_DIR.rglob(\"*.xls*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3d9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the paths into a structured table \n",
    "\n",
    "file_index = pd.DataFrame([\n",
    "    {\n",
    "        \"study_id\": p.relative_to(RAW_DIR).parts[0],\n",
    "        \"rel_path\": str(p.relative_to(RAW_DIR)),\n",
    "        \"filename\": p.name,\n",
    "        \"ext\": p.suffix.lower(),\n",
    "    }\n",
    "    for p in excel_files\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22c49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Excel files across 4 studies\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "study_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rel_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ext",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ccc6c401-3c86-4ce0-878f-ec5c7619ca12",
       "rows": [
        [
         "0",
         "Brooklyn",
         "Brooklyn/251028 Ritual_REPORT.xlsx",
         "251028 Ritual_REPORT.xlsx",
         ".xlsx"
        ],
        [
         "1",
         "Brooklyn",
         "Brooklyn/Brooklyn College Metadata.xlsx",
         "Brooklyn College Metadata.xlsx",
         ".xlsx"
        ],
        [
         "2",
         "DHM Pooled",
         "DHM Pooled/251028 DHM_Pooling_Monica_REPORT.xlsx",
         "251028 DHM_Pooling_Monica_REPORT.xlsx",
         ".xlsx"
        ],
        [
         "3",
         "DHM Pooled",
         "DHM Pooled/Copy of Copy of Pool Optimization Study- UCH+AUS.xlsx",
         "Copy of Copy of Pool Optimization Study- UCH+AUS.xlsx",
         ".xlsx"
        ],
        [
         "4",
         "NeoBANK",
         "NeoBANK/NeoBANK AC REPORT.xlsx",
         "NeoBANK AC REPORT.xlsx",
         ".xlsx"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>filename</th>\n",
       "      <th>ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn/251028 Ritual_REPORT.xlsx</td>\n",
       "      <td>251028 Ritual_REPORT.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn/Brooklyn College Metadata.xlsx</td>\n",
       "      <td>Brooklyn College Metadata.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DHM Pooled</td>\n",
       "      <td>DHM Pooled/251028 DHM_Pooling_Monica_REPORT.xlsx</td>\n",
       "      <td>251028 DHM_Pooling_Monica_REPORT.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DHM Pooled</td>\n",
       "      <td>DHM Pooled/Copy of Copy of Pool Optimization S...</td>\n",
       "      <td>Copy of Copy of Pool Optimization Study- UCH+A...</td>\n",
       "      <td>.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeoBANK</td>\n",
       "      <td>NeoBANK/NeoBANK AC REPORT.xlsx</td>\n",
       "      <td>NeoBANK AC REPORT.xlsx</td>\n",
       "      <td>.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     study_id                                           rel_path  \\\n",
       "0    Brooklyn                 Brooklyn/251028 Ritual_REPORT.xlsx   \n",
       "1    Brooklyn            Brooklyn/Brooklyn College Metadata.xlsx   \n",
       "2  DHM Pooled   DHM Pooled/251028 DHM_Pooling_Monica_REPORT.xlsx   \n",
       "3  DHM Pooled  DHM Pooled/Copy of Copy of Pool Optimization S...   \n",
       "4     NeoBANK                     NeoBANK/NeoBANK AC REPORT.xlsx   \n",
       "\n",
       "                                            filename    ext  \n",
       "0                          251028 Ritual_REPORT.xlsx  .xlsx  \n",
       "1                     Brooklyn College Metadata.xlsx  .xlsx  \n",
       "2              251028 DHM_Pooling_Monica_REPORT.xlsx  .xlsx  \n",
       "3  Copy of Copy of Pool Optimization Study- UCH+A...  .xlsx  \n",
       "4                             NeoBANK AC REPORT.xlsx  .xlsx  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# early sanity check\n",
    "\n",
    "print(f\"Found {len(file_index)} Excel files across {file_index['study_id'].nunique()} studies\")\n",
    "file_index.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47705eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filename keyword rules \n",
    "\n",
    "# 'postiive' signals\n",
    "META_KWS = [\n",
    "    \"meta\", \"metadata\", \"character\", \"clinical\", \"demograph\", \"demo\",\n",
    "    \"participant\", \"subject\", \"phenotype\", \"intake\", \"enrollment\",\n",
    "    \"questionnaire\", \"survey\", \"crf\", \"case\"\n",
    "]\n",
    "\n",
    "# 'negative' signals\n",
    "HMO_KWS = [\n",
    "    \"hmo\", \"oligo\", \"oligosacchaeride\", \"hmos\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca59ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to find keyword hits within the filenames \n",
    "# converts text to lowercase, returns a list of keywords that appear as substrings \n",
    "\n",
    "def keyword_hits(text: str, keywords: list[str]) -> list[str]:\n",
    "    text = text.lower()\n",
    "    return [kw for kw in keywords if kw in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca43a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your file index and classify \"metadata-likey by name\"\n",
    "rows = []\n",
    "\n",
    "for p in excel_files:\n",
    "    rel = p.relative_to(RAW_DIR)                  # e.g. \"Oxford/metadata.xlsx\"\n",
    "    study_id = rel.parts[0]                       # first folder name = StudyID\n",
    "    filename = p.name.lower()\n",
    "\n",
    "    meta_hits = keyword_hits(filename, META_KWS)  # positive hits\n",
    "    hmo_hits = keyword_hits(filename, HMO_KWS)    # negative hits\n",
    "\n",
    "    # \"metadata-like\" if it has meta hits AND does NOT have hmo hits\n",
    "    filename_metadata_likely = (len(meta_hits) > 0) and (len(hmo_hits) == 0)\n",
    "\n",
    "    rows.append({\n",
    "        \"study_id\": study_id,\n",
    "        \"rel_path\": str(rel),\n",
    "        \"filename\": p.name,\n",
    "        \"meta_hits\": \",\".join(meta_hits),\n",
    "        \"hmo_hits\": \",\".join(hmo_hits),\n",
    "        \"filename_metadata_likely\": filename_metadata_likely\n",
    "    })\n",
    "\n",
    "file_index = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90625ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Excel files found: 7\n",
      "Files flagged as metadata-like by filename: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "study_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rel_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "meta_hits",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hmo_hits",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "filename_metadata_likely",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "4c504cf2-f640-4441-bafe-f855ce7f8e55",
       "rows": [
        [
         "1",
         "Brooklyn",
         "Brooklyn/Brooklyn College Metadata.xlsx",
         "Brooklyn College Metadata.xlsx",
         "meta,metadata",
         "",
         "True"
        ],
        [
         "6",
         "Oxford",
         "Oxford/metadata_overview_oxford.xlsx",
         "metadata_overview_oxford.xlsx",
         "meta,metadata",
         "",
         "True"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>filename</th>\n",
       "      <th>meta_hits</th>\n",
       "      <th>hmo_hits</th>\n",
       "      <th>filename_metadata_likely</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brooklyn/Brooklyn College Metadata.xlsx</td>\n",
       "      <td>Brooklyn College Metadata.xlsx</td>\n",
       "      <td>meta,metadata</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oxford</td>\n",
       "      <td>Oxford/metadata_overview_oxford.xlsx</td>\n",
       "      <td>metadata_overview_oxford.xlsx</td>\n",
       "      <td>meta,metadata</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                 rel_path  \\\n",
       "1  Brooklyn  Brooklyn/Brooklyn College Metadata.xlsx   \n",
       "6    Oxford     Oxford/metadata_overview_oxford.xlsx   \n",
       "\n",
       "                         filename      meta_hits hmo_hits  \\\n",
       "1  Brooklyn College Metadata.xlsx  meta,metadata            \n",
       "6   metadata_overview_oxford.xlsx  meta,metadata            \n",
       "\n",
       "   filename_metadata_likely  \n",
       "1                      True  \n",
       "6                      True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect what got flagged \n",
    "\n",
    "print(\"Total Excel files found:\", len(file_index))\n",
    "print(\"Files flagged as metadata-like by filename:\", file_index[\"filename_metadata_likely\"].sum())\n",
    "\n",
    "file_index[file_index[\"filename_metadata_likely\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c9d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_DIR = Path(\"catalog\")\n",
    "CATALOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "file_index.to_csv(CATALOG_DIR / \"metadata_detection_file.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb6762",
   "metadata": {},
   "source": [
    "#### 2. Clean and Standardize Identified Metadata Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb99090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up directories\n",
    "\n",
    "RAW_DIR = Path(\"raw\")\n",
    "STAGING_DIR = Path(\"staging\")\n",
    "STAGING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CATALOG_DIR = Path(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bec12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = pd.read_csv(CATALOG_DIR / \"metadata_detection_file.csv\")\n",
    "meta_files = det.loc[det[\"filename_metadata_likely\"] == True, \"rel_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4468b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a column-name normalizer \n",
    "\n",
    "def normalize_col(c: str) -> str:\n",
    "    c = str(c).strip().lower()\n",
    "    c = re.sub(r\"\\s+\", \"_\", c)          # spaces -> underscores\n",
    "    c = re.sub(r\"[^a-z0-9_]+\", \"\", c)   # remove weird symbols\n",
    "    c = re.sub(r\"_+\", \"_\", c)           # collapse repeated underscores\n",
    "    return c.strip(\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405cb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage one file (read + clean structure + save)\n",
    "# loads the workbook, picks a sheet, reads the data, removes truly empty rows/cols, normalizes headers, writes a staged CSV under staging/metadata, returns a log row\n",
    "\n",
    "def stage_metadata_file(rel_path: str) -> dict:\n",
    "    src = RAW_DIR / rel_path\n",
    "    study_id = Path(rel_path).parts[0]\n",
    "\n",
    "    xl = pd.ExcelFile(src)                 # optionally: pd.ExcelFile(src, engine=\"openpyxl\")\n",
    "    sheet = xl.sheet_names[0]              # ok for prototype\n",
    "\n",
    "    # Read as strings to preserve IDs exactly\n",
    "    df = xl.parse(sheet_name=sheet, dtype=str)\n",
    "\n",
    "    # Drop fully empty rows/cols\n",
    "    df = df.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = [normalize_col(c) for c in df.columns]\n",
    "\n",
    "    # Strip whitespace in all string cells (prevents join failures)\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    out_dir = STAGING_DIR / study_id\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"metadata__{Path(rel_path).stem}__{normalize_col(sheet)}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    return {\n",
    "        \"study_id\": study_id,\n",
    "        \"raw_rel_path\": rel_path,\n",
    "        \"sheet_used\": sheet,\n",
    "        \"n_sheets\": len(xl.sheet_names),\n",
    "        \"sheet_names\": \"|\".join(xl.sheet_names),\n",
    "        \"rows\": df.shape[0],\n",
    "        \"cols\": df.shape[1],\n",
    "        \"staged_csv_rel_path\": str(out_path),\n",
    "        \"status\": \"success\",\n",
    "        \"error\": \"\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494fd1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged metadata files: 2\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# loops through all metadata files, stages each one, captures failures without killing the run, wrotes catalog/metadata_staging_log.csv.\n",
    "\n",
    "log_rows = []\n",
    "\n",
    "for rel_path in meta_files:\n",
    "    try:\n",
    "        log_rows.append(stage_metadata_file(rel_path))\n",
    "    except Exception as e:\n",
    "        log_rows.append({\n",
    "            \"study_id\": Path(rel_path).parts[0],\n",
    "            \"raw_rel_path\": rel_path,\n",
    "            \"sheet_used\": \"\",\n",
    "            \"rows\": None,\n",
    "            \"cols\": None,\n",
    "            \"staged_csv_rel_path\": \"\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "stage_log = pd.DataFrame(log_rows)\n",
    "stage_log.to_csv(CATALOG_DIR / \"metadata_staging_log.csv\", index=False)\n",
    "\n",
    "print(\"Staged metadata files:\", (stage_log[\"status\"] == \"success\").sum())\n",
    "print(\"Failed:\", (stage_log[\"status\"] == \"failed\").sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8a4f1",
   "metadata": {},
   "source": [
    "#### Step 3. Identify Canidate IDs and Priority Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ee1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keyword sets \n",
    "\n",
    "ID_SAMPLE_KWS = [\n",
    "    \"sample_id\", \"sampleid\", \"sample_name\", \"samplename\",\n",
    "    \"specimen\", \"aliquot\", \"barcode\", \"tube\", \"vial\", \"label\"\n",
    "]\n",
    "\n",
    "ID_SUBJECT_KWS = [\n",
    "    \"subject\", \"participant\", \"patient\"\n",
    "]\n",
    "\n",
    "PRIORITY_META_KWS = {\n",
    "    \"study_week\": [\"study_week\", \"studyweek\", \"visit_week\", \"timepoint\", \"wk\"],\n",
    "    \"maternal_age\": [\"maternal_age\", \"mat_age\", \"mother_age\", \"mom_age\", \"age_mom\"],\n",
    "    \"gestational_age_weeks\": [\"gestational\", \"ga\", \"gest_age\", \"gestation\"],\n",
    "    \"lactation_week_postpartum\": [\"postpartum\", \"pp\", \"lactation\", \"weeks_postpartum\", \"week_postpartum\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "746cc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify canidate columns in one staged metadata file \n",
    "\n",
    "\n",
    "# defines a function that takes a df and returns a dictionary of results\n",
    "def identify_candidate_columns(df: pd.DataFrame) -> dict:\n",
    "    \n",
    "    # grabs the df column names and turns into a list\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    # loops through eevery column name and keeps if any sample-keyword fro ID_SAMPLE_KWS is found as a substring\n",
    "    sample_id_candidates = [\n",
    "        c for c in cols if any(k in c for k in ID_SAMPLE_KWS)\n",
    "    ]\n",
    "\n",
    "    subject_id_candidates = [\n",
    "        c for c in cols if any(k in c for k in ID_SUBJECT_KWS)\n",
    "    ]\n",
    "\n",
    "    # builds a dictionary where each canonical priority field (e.g maternal age) maps to the list of coluns whose name contain any of the field's keywords \n",
    "    priority_hits = {\n",
    "        canon: [c for c in cols if any(k in c for k in kws)]\n",
    "        for canon, kws in PRIORITY_META_KWS.items()\n",
    "    }\n",
    "\n",
    "    # bundles three outputs into one dictionary and returns it\n",
    "    return {\n",
    "        \"sample_id_candidates\": sample_id_candidates,\n",
    "        \"subject_id_candidates\": subject_id_candidates,\n",
    "        \"priority_metadata_hits\": priority_hits\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2d0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an empty list that will store one dictionary per staged metadata file\n",
    "candidate_logs = []\n",
    "\n",
    "# loops through each row of stage_log (see code above, this is the log of all staged metadata files)\n",
    "# r is a series representing one staged file's log entry\n",
    "# _ is the row index (not used)\n",
    "\n",
    "for _, r in stage_log.iterrows():\n",
    "    if r[\"status\"] != \"success\":           #skips any file that failed staging\n",
    "        continue\n",
    "\n",
    "    # converts the staged CSV path string into a Path object so it's easier to work with\n",
    "    staged_path = Path(r[\"staged_csv_rel_path\"])\n",
    "    # reads the staged CSV into a dataframe, all columns as strings (preserves IDs exactly)\n",
    "    df = pd.read_csv(staged_path, dtype=str)\n",
    "\n",
    "    # run your fxn above to detect columns and store results in a dictionary canidates \n",
    "    candidates = identify_candidate_columns(df)\n",
    "\n",
    "    # dictionary that will become one row in output log -> study id and which CSV it came from\n",
    "    row = {\n",
    "        \"study_id\": r[\"study_id\"],\n",
    "        \"staged_csv\": r[\"staged_csv_rel_path\"],\n",
    "        \"sample_id_candidates\": \",\".join(candidates[\"sample_id_candidates\"]) or \"NA\",     # converts a list into a comma-separated string, or \"NA\" if empty\n",
    "        \"subject_id_candidates\": \",\".join(candidates[\"subject_id_candidates\"]) or \"NA\",\n",
    "    }\n",
    "\n",
    "    # one column per priority metadata field, e.g \"maternal_age\",hits=['mat_age', 'mother_age']\n",
    "    for canon, hits in candidates[\"priority_metadata_hits\"].items():\n",
    "        row[canon] = \",\".join(hits) if hits else \"NA\"\n",
    "\n",
    "    # adds the completed dictionary to the canidate_logs list \n",
    "    candidate_logs.append(row)\n",
    "\n",
    "# saves the df to catalog/metadata_candidate_columns_log.csv\n",
    "candidate_log_df = pd.DataFrame(candidate_logs)\n",
    "candidate_log_df.to_csv(\n",
    "    CATALOG_DIR / \"metadata_candidate_columns_log.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb8c12f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "study_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "staged_csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sample_id_candidates",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "subject_id_candidates",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "study_week",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "maternal_age",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "gestational_age_weeks",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lactation_week_postpartum",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "0c3e0c0a-03c0-4d27-aa45-a3073a722e3c",
       "rows": [
        [
         "0",
         "Brooklyn",
         "staging/Brooklyn/metadata__Brooklyn College Metadata__sheet1.csv",
         "sample_name",
         "subject_id",
         "study_week",
         "maternal_age",
         "gestational_age_week",
         "lactation_stage_week_postpartum"
        ],
        [
         "1",
         "Oxford",
         "staging/Oxford/metadata__metadata_overview_oxford__sheet1.csv",
         null,
         "participant_id",
         null,
         "mat_age",
         "sga,gest_age_birth,gest_age_baseline,gestational_diabetes",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>staged_csv</th>\n",
       "      <th>sample_id_candidates</th>\n",
       "      <th>subject_id_candidates</th>\n",
       "      <th>study_week</th>\n",
       "      <th>maternal_age</th>\n",
       "      <th>gestational_age_weeks</th>\n",
       "      <th>lactation_week_postpartum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>staging/Brooklyn/metadata__Brooklyn College Me...</td>\n",
       "      <td>sample_name</td>\n",
       "      <td>subject_id</td>\n",
       "      <td>study_week</td>\n",
       "      <td>maternal_age</td>\n",
       "      <td>gestational_age_week</td>\n",
       "      <td>lactation_stage_week_postpartum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oxford</td>\n",
       "      <td>staging/Oxford/metadata__metadata_overview_oxf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>participant_id</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mat_age</td>\n",
       "      <td>sga,gest_age_birth,gest_age_baseline,gestation...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                         staged_csv  \\\n",
       "0  Brooklyn  staging/Brooklyn/metadata__Brooklyn College Me...   \n",
       "1    Oxford  staging/Oxford/metadata__metadata_overview_oxf...   \n",
       "\n",
       "  sample_id_candidates subject_id_candidates  study_week  maternal_age  \\\n",
       "0          sample_name            subject_id  study_week  maternal_age   \n",
       "1                  NaN        participant_id         NaN       mat_age   \n",
       "\n",
       "                               gestational_age_weeks  \\\n",
       "0                               gestational_age_week   \n",
       "1  sga,gest_age_birth,gest_age_baseline,gestation...   \n",
       "\n",
       "         lactation_week_postpartum  \n",
       "0  lactation_stage_week_postpartum  \n",
       "1                              NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"catalog/metadata_candidate_columns_log.csv\").head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d0837",
   "metadata": {},
   "source": [
    "#### Helper Functions: 1) If multiple hits for the columns, pick..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ddde24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_TOKENS = {\n",
    "    \"gestational_age_weeks\": [\"birth\", \"delivery\"],\n",
    "    \"lactation_week_postpartum\": [\"postpartum\", \"pp\"],\n",
    "}\n",
    "\n",
    "BAD_TOKENS = {\n",
    "    \"gestational_age_weeks\": [\"baseline\", \"followup\", \"outcome\", \"score\"],\n",
    "}\n",
    "\n",
    "def pick_best_hit(canon, hits):\n",
    "    # returns best hit, alternates, best_score\n",
    "    # if empty -> None, []\n",
    "\n",
    "    if not hits:\n",
    "        return None, []\n",
    "\n",
    "    def score(col):\n",
    "        s = 0\n",
    "        col = col.lower()\n",
    "        for t in GOOD_TOKENS.get(canon, []):\n",
    "            if t in col: s += 3\n",
    "        for t in BAD_TOKENS.get(canon, []):\n",
    "            if t in col: s -= 3\n",
    "        return s\n",
    "\n",
    "    ranked = sorted(hits, key=score, reverse=True)\n",
    "    best = ranked[0]\n",
    "    best_score = score(best)\n",
    "    alts = ranked[1:]\n",
    "    return best, alts, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b52febf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Core metadata built for: 2 studies\n"
     ]
    }
   ],
   "source": [
    "all_resolution_logs = []\n",
    "all_core_outputs = []\n",
    "\n",
    "for _, r in stage_log.iterrows():\n",
    "    if r[\"status\"] != \"success\":\n",
    "        continue\n",
    "\n",
    "    study_id = r[\"study_id\"]\n",
    "    staged_csv = Path(r[\"staged_csv_rel_path\"])\n",
    "\n",
    "    # load staged metadata\n",
    "    df = pd.read_csv(staged_csv, dtype=str)\n",
    "    candidates = identify_candidate_columns(df)\n",
    "\n",
    "    core_row = {}\n",
    "    resolution_log = {}\n",
    "\n",
    "    # ---- YOUR EXISTING LOGIC (unchanged) ----\n",
    "    for canon, hits in candidates[\"priority_metadata_hits\"].items():\n",
    "        if len(hits) == 1:\n",
    "            core_row[canon] = df[hits[0]]\n",
    "            resolution_log[canon] = {\n",
    "                \"selected\": hits[0],\n",
    "                \"reason\": \"single_hit\"\n",
    "            }\n",
    "\n",
    "        elif len(hits) > 1:\n",
    "            best, alts, best_score = pick_best_hit(canon, hits)\n",
    "            core_row[canon] = df[best] if best else pd.NA\n",
    "            resolution_log[canon] = {\n",
    "                \"selected\": best,\n",
    "                \"alternates\": alts,\n",
    "                \"reason\": \"token_scoring\"\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            core_row[canon] = pd.NA\n",
    "            resolution_log[canon] = {\n",
    "                \"selected\": None,\n",
    "                \"reason\": \"not_found\"\n",
    "            }\n",
    "\n",
    "# ---------- SECTION (ID COLUMNS) ----------\n",
    "    sub_hits = candidates.get(\"subject_id_candidates\", [])\n",
    "    samp_hits = candidates.get(\"sample_id_candidates\", [])\n",
    "\n",
    "    subject_col = sub_hits[0] if len(sub_hits) > 0 else None\n",
    "    sample_col  = samp_hits[0] if len(samp_hits) > 0 else None\n",
    "\n",
    "    core_row[\"subject_id\"] = df[subject_col] if subject_col else pd.NA\n",
    "    core_row[\"hmo_sample_name\"] = df[sample_col] if sample_col else pd.NA\n",
    "\n",
    "    resolution_log[\"subject_id\"] = {\n",
    "        \"selected\": subject_col,\n",
    "        \"alternates\": sub_hits[1:] if len(sub_hits) > 1 else [],\n",
    "        \"reason\": \"id_candidate_first\"\n",
    "    }\n",
    "\n",
    "    resolution_log[\"hmo_sample_name\"] = {\n",
    "        \"selected\": sample_col,\n",
    "        \"alternates\": samp_hits[1:] if len(samp_hits) > 1 else [],\n",
    "        \"reason\": \"id_candidate_first\"\n",
    "    }\n",
    "\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # build core dataframe\n",
    "    core_df = pd.DataFrame(core_row)\n",
    "\n",
    "    # save per-study core metadata\n",
    "    out_path = Path(\"staging\") / study_id / f\"metadata__core_cleaned_{study_id}.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    core_df.to_csv(out_path, index=False)\n",
    "\n",
    "    # store logs\n",
    "    for field, info in resolution_log.items():\n",
    "        all_resolution_logs.append({\n",
    "            \"study_id\": study_id,\n",
    "            \"staged_csv\": str(staged_csv),\n",
    "            \"canonical_field\": field,\n",
    "            \"selected_column\": info.get(\"selected\"),\n",
    "            \"alternates\": \",\".join(info.get(\"alternates\", [])),\n",
    "            \"reason\": info.get(\"reason\")\n",
    "        })\n",
    "\n",
    "    all_core_outputs.append({\n",
    "        \"study_id\": study_id,\n",
    "        \"core_output\": str(out_path)\n",
    "    })\n",
    "\n",
    "# write catalog logs\n",
    "pd.DataFrame(all_resolution_logs).to_csv(\n",
    "    \"catalog/metadata_core_resolution_log.csv\", index=False\n",
    ")\n",
    "pd.DataFrame(all_core_outputs).to_csv(\n",
    "    \"catalog/metadata_core_outputs.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"Done. Core metadata built for:\", len(all_core_outputs), \"studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ff2e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DIR = Path(\"staging\")\n",
    "\n",
    "for study_id in stage_log.loc[stage_log[\"status\"]==\"success\", \"study_id\"].unique():\n",
    "\n",
    "    hmo_path = STAGING_DIR / study_id / f\"hmo_staged_{study_id}.csv\"\n",
    "    meta_path = STAGING_DIR / study_id / f\"metadata__core_cleaned_{study_id}.csv\"\n",
    "\n",
    "    if not hmo_path.exists() or not meta_path.exists():\n",
    "        continue\n",
    "\n",
    "    hmo = pd.read_csv(hmo_path, dtype=str)\n",
    "    meta = pd.read_csv(meta_path, dtype=str)\n",
    "\n",
    "    # decide merge key\n",
    "    if \"hmo_sample_name\" in meta.columns and meta[\"hmo_sample_name\"].notna().any():\n",
    "        merged = hmo.merge(\n",
    "            meta,\n",
    "            left_on=\"sample_name\",          # HMO column\n",
    "            right_on=\"hmo_sample_name\",     # metadata column\n",
    "            how=\"left\"\n",
    "        )\n",
    "        merge_key = \"sample_name\"\n",
    "\n",
    "    else:\n",
    "        merged = hmo.merge(\n",
    "            meta,\n",
    "            on=\"subject_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        merge_key = \"subject_id\"\n",
    "\n",
    "    out_path = STAGING_DIR / study_id / f\"hmo_plus_metadata_{study_id}.csv\"\n",
    "    merged.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"{study_id}: merged on {merge_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185076c0",
   "metadata": {},
   "source": [
    "### Merge the Cleaned Metadata and hmo_mereged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c79fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered studies: ['Brooklyn', 'Oxford']\n"
     ]
    }
   ],
   "source": [
    "# Discover fles by searching metadata__core_cleaned_*.csv files \n",
    "# infer StudyID from the filename (metadata__core_cleaned_{study_id}.csv)\n",
    "# load each file into a dictionary\n",
    "# for each study, auto-pick the join column -> try hmo_sammple_name vs subject_id by coverage, match rate against HMO within that StudyID, uniqueness \n",
    "# build sample_key = StudyID + \"_\" + <chosen_id>\n",
    "\n",
    "\n",
    "def discover_core_cleaned_metadata(root: str | Path) -> dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Finds metadata__core_cleaned_*.csv recursively and returns {StudyID: filepath}.\n",
    "    Guardrails:\n",
    "      - errors if multiple files map to same StudyID\n",
    "      - errors if StudyID cannot be parsed\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    paths = list(root.rglob(\"metadata__core_cleaned_*.csv\"))\n",
    "\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No files found matching metadata__core_cleaned_*.csv under: {root}\")\n",
    "\n",
    "    by_study: dict[str, Path] = {}\n",
    "    collisions: dict[str, list[Path]] = {}\n",
    "\n",
    "    pat = re.compile(r\"metadata__core_cleaned_(.+)\\.csv$\", re.IGNORECASE)\n",
    "\n",
    "    for p in paths:\n",
    "        m = pat.search(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        study_id = m.group(1).strip()\n",
    "\n",
    "        # Guard: empty or weird study_id\n",
    "        if not study_id:\n",
    "            raise ValueError(f\"Could not parse StudyID from filename: {p.name}\")\n",
    "\n",
    "        if study_id in by_study:\n",
    "            collisions.setdefault(study_id, [by_study[study_id]]).append(p)\n",
    "        else:\n",
    "            by_study[study_id] = p\n",
    "\n",
    "    if collisions:\n",
    "        msg = \"\\n\".join([f\"{sid}: \" + \", \".join(str(x) for x in ps) for sid, ps in collisions.items()])\n",
    "        raise ValueError(f\"Multiple metadata files found for same StudyID. Resolve duplicates:\\n{msg}\")\n",
    "\n",
    "    return by_study\n",
    "\n",
    "\n",
    "# Example use:\n",
    "meta_files = discover_core_cleaned_metadata(root=\".\")\n",
    "meta_by_study = {sid: pd.read_csv(path, dtype=str) for sid, path in meta_files.items()}\n",
    "print(\"Discovered studies:\", sorted(meta_by_study.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a0e601",
   "metadata": {},
   "source": [
    "### Merge Cleaned Metadata and hmo_merged\n",
    "- Load hmo_merged dataset: each row represents one milk sample identified by StudyID, SampleName\n",
    "\n",
    "- Discover cleaned metadata files automatically: searches for the project dictonary for the files nameed metadata__core_cleaned_<StudyID>.csv\n",
    "- Evaluate possible join identifiers per study by scoring using coverage, match rate, and uniqueness \n",
    "- Create a unified join key: sample_key = StudyID + \"__\" + SampleName\n",
    "- Merge metadata onto the HMO dataset Study Id and SampleName -> left join, all HMO samples are preserved, metadata columns added where available, samples without metadata remain present within empty fields \n",
    "- Sanity check: row count is unchanged, metadata attachment rate per study, all merge decisions are written to a log for transparency and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/kspann/Desktop/HMO Power Bi\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1) Load HMO merged ----------\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "hmo_path = PROJECT_ROOT / \"staging\" / \"_merged\" / \"hmo_merged.csv\"\n",
    "if not hmo_path.exists():\n",
    "    raise FileNotFoundError(f\"HMO merged file not found at: {hmo_path}\")\n",
    "\n",
    "hmo = pd.read_csv(hmo_path, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SANITY CHECKS ===\n",
      "HMO rows (before): 1200\n",
      "HMO rows (after) : 1200\n",
      "\n",
      "Metadata attached rate by StudyID:\n",
      "StudyID\n",
      "Oxford        0.990385\n",
      "Brooklyn      0.946154\n",
      "DHM Pooled    0.000000\n",
      "NeoBANK       0.000000\n",
      "Name: has_meta, dtype: float64\n",
      "\n",
      "Wrote:\n",
      " - derived/metadata_master.csv\n",
      " - derived/hmo_merged_with_metadata.csv\n",
      " - derived/metadata_merge_log.json\n"
     ]
    }
   ],
   "source": [
    "# Basic hygiene\n",
    "# standardize join columns and strip whitespace to avoid space mismatches\n",
    "hmo[\"StudyID\"] = hmo[\"StudyID\"].astype(str).str.strip()\n",
    "hmo[\"SampleName\"] = hmo[\"SampleName\"].astype(str).str.strip()\n",
    "\n",
    "# We'll use these per study for matching\n",
    "# builds a dictionary like {Brooklyn\": {1_wk0, 1_wk10,....}} to score metadata cnanidate columns within each study\n",
    "hmo_names_by_study = {\n",
    "    sid: set(hmo.loc[hmo[\"StudyID\"] == sid, \"SampleName\"].dropna().astype(str).str.strip().unique())\n",
    "    for sid in hmo[\"StudyID\"].dropna().unique()\n",
    "}\n",
    "\n",
    "# ---------- 2) Helper: score candidate id columns ----------\n",
    "def score_id_column(meta_df: pd.DataFrame, hmo_sample_set: set[str], col: str) -> dict:\n",
    "    \n",
    "    # if column doesn't exist in metadata file, return early error message\n",
    "    if col not in meta_df.columns:\n",
    "        return {\"col\": col, \"usable\": False}\n",
    "\n",
    "    # pull the column out as a pandas series - s contains all values in that column (including missing values)\n",
    "    s = meta_df[col]\n",
    "    # \"what fraction of rows are NOT missing in this column?\"\n",
    "    coverage = s.notna().mean()\n",
    "\n",
    "    # remove missing values, strip whitespace, removes leading/trailing spaces; return clean series\n",
    "    s_clean = s.dropna().astype(str).str.strip()\n",
    "    if s_clean.empty:\n",
    "        return {\"col\": col, \"usable\": False, \"coverage\": float(coverage)}\n",
    "\n",
    "    # gets unique values and turns it into a set to make checks faster\n",
    "    meta_unique = set(s_clean.unique())\n",
    "\n",
    "    # counts how many unique metadata IDs are present in the HMO sample IDs -> get the proportion\n",
    "    n_match = sum(v in hmo_sample_set for v in meta_unique)\n",
    "    match_rate = n_match / max(len(meta_unique), 1)\n",
    "\n",
    "    # measures how unique that column is relative to the number of rows, if close to 1 -> very unique (sample-level), if closer to 0 -> less unique (participant level)\n",
    "    uniqueness_ratio = s_clean.nunique() / max(len(s_clean), 1)\n",
    "\n",
    "    # Score: prioritize match_rate heavily; coverage helps as tie-breaker\n",
    "    score = 0.85 * match_rate + 0.15 * coverage\n",
    "\n",
    "    return {\n",
    "        \"col\": col,\n",
    "        \"usable\": True,\n",
    "        \"coverage\": float(coverage),\n",
    "        \"meta_unique\": int(len(meta_unique)),\n",
    "        \"hmo_unique\": int(len(hmo_sample_set)),\n",
    "        \"n_match\": int(n_match),\n",
    "        \"match_rate\": float(match_rate),\n",
    "        \"uniqueness_ratio\": float(uniqueness_ratio),\n",
    "        \"score\": float(score),\n",
    "    }\n",
    "\n",
    "# DECISION LEVEL \n",
    "    # tests multiple possible ID columns (hmo_sample_name vs subject_id) and chooses the best using scoring_id^\n",
    "def choose_best_id_column(meta_df: pd.DataFrame, hmo_sample_set: set[str], candidates: list[str]) -> dict:\n",
    "    scored = [score_id_column(meta_df, hmo_sample_set, c) for c in candidates]\n",
    "    scored = [d for d in scored if d.get(\"usable\")]\n",
    "\n",
    "    if not scored:\n",
    "        return {\"chosen\": None, \"scored\": []}\n",
    "\n",
    "    scored_sorted = sorted(scored, key=lambda d: d[\"score\"], reverse=True)\n",
    "    return {\"chosen\": scored_sorted[0][\"col\"], \"scored\": scored_sorted}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 3) Load metadata (from your discovery step) ----------\n",
    "# starting with the project root, find all cleaned metadata files, label them by StudyID, load each into a DF, store them in a dict keyed by study\n",
    "# You should already have meta_files = {StudyID: Path(...)} from Option B discovery\n",
    "meta_files = discover_core_cleaned_metadata(root=\".\")  # root . means the current working dict\n",
    "\n",
    "meta_by_study = {sid: pd.read_csv(path, dtype=str) for sid, path in meta_files.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 4) Pick join column per study + build metadata_master ----------\n",
    "# list of metadata columns we're willing to try as the join identifier \n",
    "ID_CANDIDATES = [\"hmo_sample_name\", \"subject_id\", \"SampleName\", \"sample_name\", \"sample_id\"]\n",
    "\n",
    "# start a list that we will collect each study's metadata after we add a sample_key and then what we choose and why\n",
    "meta_parts = []\n",
    "merge_log = []\n",
    "\n",
    "# loop through metadata dictionary \n",
    "# sid = study name string; meta = dataframe for that study's metadata \n",
    "for sid, meta in meta_by_study.items():\n",
    "    sid_str = str(sid).strip()\n",
    "    meta.columns = [c.strip() for c in meta.columns]\n",
    "    # add StudyID column to metadata - for creating sample_key and preventing cross-study collisions\n",
    "    meta[\"StudyID\"] = sid_str\n",
    "\n",
    "    hmo_set = hmo_names_by_study.get(sid_str, set())\n",
    "\n",
    "    # runs the scoring logic across all canidate columns and pulls the winning column name or None if nothing was usable\n",
    "    choice = choose_best_id_column(meta, hmo_set, ID_CANDIDATES)\n",
    "    chosen = choice[\"chosen\"]\n",
    "\n",
    "    merge_log.append({\n",
    "        \"StudyID\": sid_str,\n",
    "        \"chosen_id_col\": chosen,\n",
    "        \"scores\": choice[\"scored\"][:3],  # keep top 3 for readability\n",
    "    })\n",
    "\n",
    "    if chosen is None:\n",
    "        print(f\"[WARN] {sid_str}: No usable ID column found. Skipping this study for metadata merge.\")\n",
    "        continue\n",
    "\n",
    "    # Build sample_key\n",
    "    meta[\"_id_\"] = meta[chosen].astype(str).str.strip()\n",
    "    meta[\"sample_key\"] = meta[\"StudyID\"].astype(str).str.strip() + \"__\" + meta[\"_id_\"]\n",
    "\n",
    "    # IMPORTANT: ensure 1 row per sample_key\n",
    "    dup_ct = meta.duplicated(\"sample_key\").sum()\n",
    "    if dup_ct > 0:\n",
    "        print(f\"[WARN] {sid_str}: {dup_ct} duplicate sample_key rows in metadata. Keeping first occurrence.\")\n",
    "        meta = meta.drop_duplicates(\"sample_key\")\n",
    "\n",
    "    meta_parts.append(meta.drop(columns=[\"_id_\"], errors=\"ignore\"))\n",
    "\n",
    "metadata_master = pd.concat(meta_parts, ignore_index=True) if meta_parts else pd.DataFrame(columns=[\"sample_key\"])\n",
    "\n",
    "# ---------- 5) Build sample_key in HMO + left merge ----------\n",
    "hmo[\"sample_key\"] = hmo[\"StudyID\"].astype(str).str.strip() + \"__\" + hmo[\"SampleName\"].astype(str).str.strip()\n",
    "\n",
    "hmo_with_meta = hmo.merge(\n",
    "    metadata_master.drop(columns=[\"StudyID\"], errors=\"ignore\"),\n",
    "    on=\"sample_key\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_meta\")\n",
    ")\n",
    "\n",
    "# ---------- 6) Sanity checks ----------\n",
    "print(\"\\n=== SANITY CHECKS ===\")\n",
    "print(\"HMO rows (before):\", len(hmo))\n",
    "print(\"HMO rows (after) :\", len(hmo_with_meta))\n",
    "\n",
    "# Per-study match rate: \"did we attach *any* metadata column?\"\n",
    "meta_cols = [c for c in hmo_with_meta.columns if c not in hmo.columns]\n",
    "if meta_cols:\n",
    "    has_meta = hmo_with_meta[meta_cols].notna().any(axis=1)\n",
    "    print(\"\\nMetadata attached rate by StudyID:\")\n",
    "    print(hmo_with_meta.assign(has_meta=has_meta).groupby(\"StudyID\")[\"has_meta\"].mean().sort_values(ascending=False))\n",
    "else:\n",
    "    print(\"[WARN] No metadata columns were merged in.\")\n",
    "\n",
    "# ---------- 7) Write outputs ----------\n",
    "Path(\"derived\").mkdir(exist_ok=True)\n",
    "metadata_master.to_csv(\"derived/metadata_master.csv\", index=False)\n",
    "hmo_with_meta.to_csv(\"derived/hmo_merged_with_metadata.csv\", index=False)\n",
    "pd.DataFrame(merge_log).to_json(\"derived/metadata_merge_log.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"\\nWrote:\")\n",
    "print(\" - derived/metadata_master.csv\")\n",
    "print(\" - derived/hmo_merged_with_metadata.csv\")\n",
    "print(\" - derived/metadata_merge_log.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global Env)",
   "language": "python",
   "name": "global-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
